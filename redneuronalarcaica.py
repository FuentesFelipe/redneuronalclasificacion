# -*- coding: utf-8 -*-
"""RedNeuronalArcaica.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15hMwJhoU1NaY-gveWZR0UleetzgZPN8l
"""

import numpy as np
import scipy as sc
import matplotlib.pyplot as plt

from sklearn.datasets import make_circles

#DATASET

n = 500 #registros que tenemos
p = 2   #caracteristicas de cada registro

X, Y = make_circles(n_samples= n, factor = 0.5, noise = 0.05) #Factor distancia entre circulos

Y = Y[:, np.newaxis]

plt.scatter(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], c="skyblue")
plt.scatter(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], c="salmon")
plt.axis("equal")
plt.show()

#Clase de la capa de la RED

class neural_layer():
  
  def __init__(self, n_conn, n_neur, act_f ):
    
    self.act_f = act_f
    
    self.b = np.random.rand(1, n_neur)      * 2 - 1 #Vector columna
    self.W = np.random.rand(n_conn, n_neur) * 2 - 1 #matriz connXneur

# Funciones de activacion

sigm = (lambda x: 1 / (1 + np.e** (-x)),
        lambda x: x * (1 - x))

relu = lambda x: np.maximum(0, x)

_x = np.linspace(-5, 5, 100)
plt.plot(_x, relu(_x))

#Red neuronal

l0 = neural_layer(p, 4, sigm)
l1 = neural_layer(4, 8, sigm)

def create_nn(topology, act_f):
  
  nn = [] # Estructura red
  
  for l, layer in enumerate(topology[:-1]): #descartamos el ultimo
    
    nn.append(neural_layer(topology[l], topology[l+1], act_f))
    
  return nn

topology = [p, 4, 8, 1]

neural_net = create_nn(topology, sigm)

l2_cost = (lambda Ypredicha, Yreal: np.mean((Ypredicha - Yreal) ** 2),
           lambda Ypredicha, Yreal: (Ypredicha - Yreal))

#learning_rate son los saltos en el descenso del gradiente

def train(neural_net, X, Y, l2_cost, learning_rate = 0.5, train=True):
  
  out = [(None,X)]
  
  # Forward Pass pasar un vector capa por capa
  # @ = multiplicacion matricial
  
  for l, layer in enumerate(neural_net):
  
    z = out[-1][1] @ neural_net[l].W + neural_net[l].b
    a = neural_net[0].act_f[0](z)
    
    out.append((z,a))
    
  
  if train:
    
    
    # Backward pass
    
    deltas = []
    for l in reversed(range(0, len(neural_net))):
      
      z = out[l+1][0]
      a = out[l+1][1]
      
      
      if l == len(neural_net) - 1:
        
        #Calcular delta Ãºltima capa.
        deltas.insert(0, l2_cost[1](a, Y) * neural_net[l].act_f[1](a))
       
      else:
        
        deltas.insert(0, deltas[0] @ _W.T * neural_net[l].act_f[1](a))
        
      _W = neural_net[l].W
    
      # Gradient descent

      neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True) * learning_rate
      neural_net[l].W = neural_net[l].W - out[l][1].T @ deltas[0] * learning_rate
      
  return out[-1][1]
  

  
train(neural_net, X, Y, l2_cost, 0.5, True)

import time
from IPython.display import clear_output

neural_nn = create_nn(topology, sigm)

loss = []

# Entrenamiento

for i in range(2500):
  
  pY = train(neural_nn, X, Y, l2_cost, learning_rate=0.05)
  
  if i % 250 == 0:
    
    print(pY)
    
    loss.append(l2_cost[0](pY,Y))
    
    res = 50
    
    _x0 = np.linspace(-1.5, 1.5, res)
    _x1 = np.linspace(-1.5, 1.5, res)
    
    _Y = np.zeros((res, res))
    
    for i0, x0 in enumerate(_x0):
      for i1, x1 in enumerate(_x1):
        _Y[i0, i1] = train(neural_nn, np.array([[x0, x1]]), Y, l2_cost, train = False)[0][0]
        
    plt.pcolormesh(_x0, _x1, _Y, cmap="coolwarm")
    plt.axis("equal")
    
    plt.scatter(X[Y[:,0] == 0, 0], X[Y[:,0] == 0, 1], c= "skyblue")
    plt.scatter(X[Y[:,0] == 1, 0], X[Y[:,0] == 1, 1], c= "salmon")

    clear_output(wait=True)
    plt.show()
    plt.plot(range(len(loss)), loss)
    plt.show()
    time.sleep(0.5)